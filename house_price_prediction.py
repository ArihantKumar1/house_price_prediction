# -*- coding: utf-8 -*-
"""Copy of House Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAqq59DC8jJPBgsw1ArIz4sFKn1j_4jS
"""
import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('train.csv')

# Display basic information about the dataset
print("Initial Data Information:")
data.info()
print("\nMissing Values:")
print(data.isna().sum())

# Drop irrelevant columns
irrelevant_columns = ['lot_size', 'lot_size_units', 'size_units']
data.drop(columns=irrelevant_columns, inplace=True)

# Handle missing values
data.fillna({
    'beds': data['beds'].median(),  # Fill missing 'beds' with median
    'baths': data['baths'].median(),  # Fill missing 'baths' with median
}, inplace=True)

# Add price per square foot as a derived feature
data['price_per_sqft'] = data['price'] * 100000 / data['size']

# Remove outliers using IQR method for continuous variables
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

continuous_columns = ['price', 'size', 'price_per_sqft']
for col in continuous_columns:
    data = remove_outliers(data, col)

categorical_columns = data.select_dtypes(include=['object']).columns

# Using one-hot encoding for simplicity
data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns.drop('price')
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Final dataset after preprocessing
print("\nFinal Data Information:")
data.info()

# Save the preprocessed dataset
data.to_csv("final_preprocessed_dataset.csv", index=False)

# Display basic statistics of the processed data
print("\nFinal Dataset Statistics:")
print(data.describe())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
import warnings

# Ignore warnings for cleaner output
warnings.filterwarnings('ignore')

# Load the preprocessed dataset
data = pd.read_csv("final_preprocessed_dataset.csv")

# Define features (X) and target (y)
X = data.drop(columns=['price'])
y = data['price']

# Apply StandardScaler and PCA to the entire feature set
scaler = StandardScaler()
pca = PCA(n_components=2)  # Reduce to 2 components for visualization

X_scaled = scaler.fit_transform(X)
X_pca = pca.fit_transform(X_scaled)

# Split the data into training and testing sets (after scaling and PCA)
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Lasso Regression": Lasso(alpha=0.1),
    "Ridge Regression": Ridge(alpha=1),
    "Decision Tree": DecisionTreeRegressor(max_depth=3, random_state=42),  # Reduced max_depth for pruning
    "KNN Regression": KNeighborsRegressor(n_neighbors=3),  # Reduced n_neighbors
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
}

# Train each model and plot relevant graphs
for name, model in models.items():
    # Create a pipeline including PCA
    pipe = make_pipeline(scaler, pca, model)

    # Train the model
    pipe.fit(X_train, y_train)

    print(f"{name} model training complete.")

    if name == "Decision Tree":
        # Plot the decision tree
        plt.figure(figsize=(20, 10))
        plot_tree(model, filled=True, feature_names=X.columns, rounded=True, fontsize=10)
        plt.title("Decision Tree Visualization")
        plt.show()

    elif name == "KNN Regression":
        # Transform test data using PCA
        X_test_pca = pca.transform(X_test)

        # Generate a grid for plotting clusters
        x_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1
        y_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))

        # Predict using KNN for grid
        Z = model.predict(pca.transform(np.c_[xx.ravel(), yy.ravel()]))
        Z = Z.reshape(xx.shape)

        # Plot the decision boundary
        plt.figure(figsize=(10, 6))
        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)
        plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, edgecolor='k', cmap=plt.cm.coolwarm)
        plt.title("KNN Regression Clusters with PCA")
        plt.xlabel("PCA Component 1")
        plt.ylabel("PCA Component 2")
        plt.colorbar(label="Predicted Price")
        plt.show()

    elif name == "Random Forest":
        # Plot feature importances for Random Forest with PCA
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        plt.figure(figsize=(12, 6))
        plt.title("Feature Importances (Random Forest with PCA)")
        plt.bar(range(pca.n_components_), importances[indices], color="skyblue", align="center")
        plt.xticks(range(pca.n_components_), [f'PCA {i+1}' for i in indices], rotation=90)
        plt.tight_layout()
        plt.show()

    elif name in ["Linear Regression", "Lasso Regression", "Ridge Regression"]:
        # Transform test data using PCA
        X_test_pca = pca.transform(X_test)
        y_test_pred = pipe.predict(X_test_pca)

        plt.figure(figsize=(8, 6))
        plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k')
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2)
        plt.title(f"Actual vs Predicted Values ({name} with PCA)")
        plt.xlabel("Actual Values")
        plt.ylabel("Predicted Values")
        plt.tight_layout()
        plt.show()

# Load the preprocessed dataset
data = pd.read_csv("final_preprocessed_dataset.csv")

# Define features (X) and target (y)
X = data.drop(columns=['price'])
y = data['price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# StandardScaler
scaler = StandardScaler()

# Define models
models = {
    "Linear Regression": LinearRegression(),
    "Lasso Regression": Lasso(alpha=0.1),
    "Ridge Regression": Ridge(alpha=1),
    "Decision Tree": DecisionTreeRegressor(max_depth=5, random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
    "KNN Regression": KNeighborsRegressor(n_neighbors=5)
}

# Train models and print predictions
results = {}

for name, model in models.items():
    if name == "Linear Regression":
        # No need for pipeline with PCA or scaling for Linear Regression
        model.fit(X_train, y_train)
        y_test_pred = model.predict(X_test)
    else:
        # For other models, include scaling and/or PCA
        pipe = make_pipeline(scaler, model)
        pipe.fit(X_train, y_train)
        y_test_pred = pipe.predict(X_test)

    # Store the results
    results[name] = {
        "MAE": np.mean(np.abs(y_test - y_test_pred)),
        "MSE": np.mean((y_test - y_test_pred) ** 2),
        "RMSE": np.sqrt(np.mean((y_test - y_test_pred) ** 2)),
        "R2 Score": model.score(X_test, y_test)
    }

    print(f"--- {name} ---")
    print(f"Actual values (first 10): {y_test.values[:10]}")
    print(f"Predicted values (first 10): {y_test_pred[:10]}")
    print(f"Mean Absolute Error: {results[name]['MAE']:.2f}")
    print(f"Mean Squared Error: {results[name]['MSE']:.2f}")
    print(f"Root Mean Squared Error: {results[name]['RMSE']:.2f}")
    print(f"R^2 Score: {results[name]['R2 Score']:.2f}")
    print("-" * 40)

# Plotting performance comparison
methods = list(results.keys())
mae_values = [results[method]['MAE'] for method in methods]
mse_values = [results[method]['MSE'] for method in methods]
rmse_values = [results[method]['RMSE'] for method in methods]
r2_values = [results[method]['R2 Score'] for method in methods]

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.bar(methods, mae_values, color='blue', alpha=0.7)
plt.title("Mean Absolute Error (MAE)")
plt.ylabel("Error")

plt.subplot(2, 2, 2)
plt.bar(methods, mse_values, color='green', alpha=0.7)
plt.title("Mean Squared Error (MSE)")
plt.ylabel("Error")

plt.subplot(2, 2, 3)
plt.bar(methods, rmse_values, color='orange', alpha=0.7)
plt.title("Root Mean Squared Error (RMSE)")
plt.ylabel("Error")

plt.subplot(2, 2, 4)
plt.bar(methods, r2_values, color='purple', alpha=0.7)
plt.title("RÂ² Score")
plt.ylabel("Score")

plt.tight_layout()
plt.show()
with open('models.pkl', 'wb') as file:
    pickle.dump(models, file)